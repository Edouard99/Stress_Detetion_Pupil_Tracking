{"cells":[{"cell_type":"markdown","metadata":{"id":"USaorKo7DP9B"},"source":["# Base"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":48130,"status":"ok","timestamp":1646363043748,"user":{"displayName":"Edouard .C","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07320798258634684264"},"user_tz":360},"id":"gr2Cj9rxQ_NT","outputId":"3060f995-4787-4f13-803e-04369625dc7c"},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\Edouard\\github\\Stress_Detetion_Pupil_Tracking\\venv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import os\n","from torch.nn.modules.activation import LeakyReLU\n","from tqdm import tqdm\n","import torch\n","import torch.nn as nn\n","import torch.nn.parallel\n","import torch.backends.cudnn as cudnn\n","import torch.optim as optim\n","import torch.utils.data\n","from torch.utils.data import Dataset\n","import matplotlib.animation as animation\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import random\n","import json\n","from numpy import genfromtxt\n","import seaborn as sns"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["DIR_NETWORK=\"/content/network/\"\n","DIR_DATA=\"/content/data_treated/\" #Path where you store your treated data"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"9Qy3Go4cE7EZ"},"outputs":[{"data":{"text/plain":["<torch._C.Generator at 0x14dd447d050>"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["manualSeed=1\n","\n","torch.manual_seed(manualSeed)\n","random.seed(manualSeed)\n","np.random.seed(manualSeed)\n","g = torch.Generator()\n","g.manual_seed(manualSeed)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fieHkJgfvtQV"},"outputs":[],"source":["# l= os.listdir(\"/content/data\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pFEKjvFQ2d27"},"outputs":[],"source":["name_list=['data_ext_t2_experimental.txt.json', 'data_ext_t4_control.txt.json',\n"," 'data_ext_t0_experimental.txt.json', 'data_ext_t1_control.txt.json',\n"," 'data_ext_t1_experimental.txt.json', 'data_ext_t0_control.txt.json',\n"," 'data_ext_t3_control.txt.json', 'data_ext_t2_control.txt.json',\n"," 'data_ext_t4_experimental.txt.json', 'data_ext_t3_experimental.txt.json']"]},{"cell_type":"markdown","metadata":{"id":"o4lE6XdnkRCt"},"source":["# Functions"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"emSxBk7ikS5m"},"outputs":[],"source":["def conf_mat(net,datal,trsh):\n","  x=datal[0].float().to(device)\n","  y=net(x).view(-1)\n","  pred=(y>trsh).int()\n","  label=datal[1].float().to(device).view(-1).int()\n","  comp=torch.eq(label,pred).int()\n","  mat_nolbl=np.zeros((2,2))\n","  for i in range(0,2):\n","    tens=torch.where(label==i,1,0)\n","    numtot=torch.sum(tens).item()\n","    num_G=torch.sum(torch.where(torch.mul(tens,comp)==1,1,0)).item()\n","    if i ==1:\n","      mat_nolbl[0,0]+=num_G\n","      mat_nolbl[1,0]+=numtot-num_G\n","    else:\n","      mat_nolbl[1,1]+=num_G\n","      mat_nolbl[0,1]+=numtot-num_G\n","  return mat_nolbl"]},{"cell_type":"markdown","metadata":{"id":"88CdoUVQrpaA"},"source":["# DS creation kfold"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UNrIGSslSwbF"},"outputs":[],"source":["class ds_pd(Dataset):\n","    def __init__(self, list_sample,mean_global,std_global):\n","        self.samples = []\n","        mean=mean_global\n","        std=std_global\n","        for i in range(0,len(list_sample)):\n","            label=list_sample[i][1]\n","            x=(np.array(list_sample[i][0])-mean)/(std)\n","            self.samples.append((x,label))\n","\n","    def __len__(self):\n","        return len(self.samples)\n","\n","    def __getitem__(self, id):\n","        return self.samples[id]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\"\n","Create the list that will be used to generate the datasets.\n","Our data files include 14 subject from control group and 17 subject from experimental group.\n","Experimental subject 11 will be the testing subject (out of cross validation).\n","As a cross validation is used in this project, the training datasets will be made from data of 13 subject of control group + 15 subject of experimental group\n","The validation datasets will be made of 1 subject from control group and 1 subject from experimental group.\n","\n","The output is a list containing fold element. The cross validation with this configuration is a 195-fold crossvalidation.\n","A fold element is [data_for_training;data_for_validation;experimental_subject_used_for_validation_index;control_subject_used_for_validation_index]\n","\"\"\"\n","ds_list=[]\n","for v1 in range(0,16):\n","  if (v1!=11):\n","    for v2 in range(0,13):\n","      L_val=[]\n","      L_train=[]\n","      ind=0\n","      for name in name_list:\n","        f=open(DIR_DATA+name)\n","        group=name[12:-9]\n","        num=name[10:11]\n","        L = json.load(f)\n","        if group==\"control\":\n","          label=0\n","          for k in range(0,len(L)):\n","            if k==v2:\n","              for i in range(0,len(L[k])):\n","                L_val.append([L[k][i],label])\n","            else :\n","              for i in range(0,len(L[k])):\n","                L_train.append([L[k][i],label])\n","        if group==\"experimental\":\n","          if int(num)>=2:\n","            label=1\n","            ind+=1\n","          else:\n","            label=0\n","          for k in range(0,len(L)):\n","            if k==v1:\n","              for i in range(0,len(L[k])):\n","                L_val.append([L[k][i],label])\n","            else :\n","              if label==0 and k!=11:\n","                for i in range(0,len(L[k])):\n","                  L_train.append([L[k][i],label])\n","              if label==1 and k!=11:\n","                for i in range(0,len(L[k])):\n","                  L_train.append([L[k][i],label])\n","      ds_list.append([L_train,L_val,v1,v2])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bpB4PaZ-Bl38"},"outputs":[],"source":["\"\"\" Generate the 195 datasets from the list for cross-validation\"\"\"\n","\n","fold_list=[]\n","mean_t=[np.mean(a[0]) for a in ds_list[0][0]]\n","mean_v=[np.mean(a[0]) for a in ds_list[0][1]]\n","mean_global=np.mean(mean_t+mean_v)\n","\n","std_t=[np.std(a[0]) for a in ds_list[0][0]]\n","std_v=[np.std(a[0]) for a in ds_list[0][1]]\n","std_global=np.mean(std_t+std_v)\n","for ds in ds_list:\n","  ds_t=ds_pd(ds[0],mean_global,std_global)\n","  ds_v=ds_pd(ds[1],mean_global,std_global)\n","  fold_list.append([ds_t,ds_v,ds[2],ds[3]])"]},{"cell_type":"markdown","metadata":{"id":"VZVLgHmjVhGa"},"source":["# Model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":73},"executionInfo":{"elapsed":22,"status":"ok","timestamp":1646363093382,"user":{"displayName":"Edouard .C","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07320798258634684264"},"user_tz":360},"id":"PN7Ya-GyVgDe","outputId":"24fee334-9c16-4593-b9af-9c7ae09b999f"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["''"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["\"\"\" DNN Model \"\"\"\n","\n","\n","def init_weight(m):\n","    \"\"\"Initialization of the weights\"\"\"\n","    if isinstance(m,nn.Linear):\n","        nn.init.xavier_uniform_(m.weight)\n","        m.bias.data.fill_(0.01)\n","    if isinstance(m, nn.BatchNorm1d):\n","        m.weight.data.fill_(1)\n","        m.bias.data.zero_()\n","\n","\n","class Classifier_PD(nn.Module):\n","    \"\"\"DNN model, see the model architecture in the report for more details\"\"\"\n","    def __init__(self, ngpu):\n","        super(Classifier_PD, self).__init__()\n","        self.ngpu = ngpu\n","        self.nnPD = nn.Sequential(\n","            nn.Linear(125,64,bias=True),\n","            nn.BatchNorm1d(64),\n","            nn.Dropout(0.4),\n","            nn.LeakyReLU(0.2),\n","            nn.Linear(64,32,bias=True),\n","            nn.BatchNorm1d(32),\n","            nn.Dropout(0.4),\n","            nn.LeakyReLU(0.2),\n","            nn.Linear(32,8,bias=True),\n","            nn.BatchNorm1d(8),\n","            nn.Dropout(0.4),\n","            nn.LeakyReLU(0.2),\n","            nn.Linear(8,1,bias=True),\n","            nn.Sigmoid()\n","        )\n","        self.nnPD.apply(init_weight)\n","\n","    def forward(self, input):\n","        return self.nnPD(input)    "]},{"cell_type":"markdown","metadata":{"id":"B6BCMV8WQmJt"},"source":["# Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HvpAkUbPWJNi"},"outputs":[],"source":["def training(net,dataloader_t,dataloader_v,num_epochs,j,k):\n","  \"\"\" Training with a BCELoss on the dataset, for each epoch the net weights are saved and the mean error is computed to plot the loss\n","  for training and valdiation dataset\n","  The training is done using data augmentation with noise, a learning rate scheduler and checkpoints\n","  At each epoch network and optimizer data are saved in DIR_NETWORK repository\n","  \"\"\"\n","  Loss = []\n","  Lossv= []\n","  lr=0.0001\n","  beta1=0.9\n","  optimizer = optim.Adam(net.parameters(), lr=lr, betas=(beta1, 0.999),weight_decay=0.005)\n","  scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min',factor=0.5,patience=15)\n","  temp_lr=lr\n","  temp_best_loss=100000\n","  for epoch in range(num_epochs):\n","      L_t=[]\n","      L_v=[]\n","      for i, dataj in enumerate(dataloader_t, 0):\n","          net.zero_grad()\n","          x=dataj[0].float().to(device)\n","          noise=np.sqrt(0.008)*torch.randn(x.shape).float().to(\"cuda\")\n","          xnoise=x+noise\n","          yhat=dataj[1].float().to(device)\n","          yhat=yhat.view(-1,1)\n","          y=net(x)\n","          err_t=nn.BCELoss()(y.float(),yhat.float())\n","          err_t.backward()\n","          optimizer.step()\n","          L_t.append(err_t.item())\n","      for i, dataj in enumerate(dataloader_v, 0):\n","        net.eval()     \n","        x=dataj[0].float().to(device)\n","        yhat=dataj[1].float().to(device)\n","        yhat=yhat.view(-1,1)\n","        y=net(x)\n","        err_v=nn.BCELoss()(y.float(),yhat.float())\n","        L_v.append(err_v.item())\n","      err=np.mean(L_t)\n","      errv=np.mean(L_v)\n","      Loss.append(err)\n","      Lossv.append(errv)\n","      torch.save(net.state_dict(), DIR_NETWORK+\"net_\"+str(j)+\"_\"+str(k)+\"/net_\"+str(j)+\"_\"+str(k)+\"_epoch_\"+str(epoch)+\".pth\")\n","      torch.save(optimizer.state_dict(), DIR_NETWORK+\"optim_\"+str(j)+\"_\"+str(k)+\"/optim_\"+str(j)+\"_\"+str(k)+\"_epoch_\"+str(epoch)+\".pth\")\n","      if temp_best_loss>errv:\n","        temp_best_loss=errv\n","        temp_best_epoch=epoch\n","        #print(\"update best : {}\".format(temp_best_epoch))\n","      scheduler.step(errv)\n","      if temp_lr>optimizer.state_dict()[\"param_groups\"][0][\"lr\"]:\n","        temp_lr=optimizer.state_dict()[\"param_groups\"][0][\"lr\"]\n","        net.load_state_dict(torch.load(DIR_NETWORK+\"net_\"+str(j)+\"_\"+str(k)+\"/net_\"+str(j)+\"_\"+str(k)+\"_epoch_\"+str(temp_best_epoch)+\".pth\"))\n","        dict_optim=torch.load(DIR_NETWORK+\"optim_\"+str(j)+\"_\"+str(k)+\"/optim_\"+str(j)+\"_\"+str(k)+\"_epoch_\"+str(temp_best_epoch)+\".pth\")\n","        dict_optim[\"param_groups\"][0][\"lr\"]=temp_lr\n","        optimizer.load_state_dict(dict_optim)\n","\n","  return [Loss,Lossv,np.argmin(Lossv)]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6946182,"status":"ok","timestamp":1646370039547,"user":{"displayName":"Edouard .C","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07320798258634684264"},"user_tz":360},"id":"tKLXZEHUZ5Jd","outputId":"aae10d4d-132f-4f59-b41c-6ee5d24e9e5e"},"outputs":[],"source":["def seed_worker(worker_id):\n","    worker_seed = torch.initial_seed() % 2**32\n","    np.random.seed(worker_seed)\n","    random.seed(worker_seed)\n","\n","class NpEncoder(json.JSONEncoder):\n","  def default(self, obj):\n","      if isinstance(obj, np.integer):\n","          return int(obj)\n","      if isinstance(obj, np.floating):\n","          return float(obj)\n","      if isinstance(obj, np.ndarray):\n","          return obj.tolist()\n","      return super(NpEncoder, self).default(obj)\n","\n","\n","num_workers = 2\n","batch_size = 256\n","ngpu = 1\n","device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n","\n","L=[]    \n","cntr=0\n","M=range(len(fold_list)//2,3*(len(fold_list)//4),1)\n","for iteration in tqdm(M):\n","  ds=fold_list[iteration]\n","  net= Classifier_PD(ngpu).to(device)\n","  dataset_t=ds[0]\n","  dataset_v=ds[1]\n","  k=ds[2]\n","  j=ds[3]\n","  os.mkdir(DIR_NETWORK+\"net_{}_{}\".format(j,k))\n","  os.mkdir(DIR_NETWORK+\"optim_{}_{}\".format(j,k))\n","  dataloader_t = torch.utils.data.DataLoader(dataset_t,batch_size=batch_size,shuffle=True,num_workers=num_workers,worker_init_fn=seed_worker,generator=g, drop_last=True)\n","  dataloader_v = torch.utils.data.DataLoader(dataset_v,batch_size=batch_size,shuffle=True,num_workers=num_workers,worker_init_fn=seed_worker,generator=g, drop_last=True)\n","  L=training(net,dataloader_t,dataloader_v,250,j,k)\n","  with open(DIR_NETWORK+\"results_{}_{}.json\".format(j,k),\"w\") as file:\n","    json.dump(L,file,cls=NpEncoder)\n","  "]},{"cell_type":"markdown","metadata":{},"source":["# Load and Results Analysis"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["confusionmean=np.zeros((2,2))\n","acc_list=[]\n","prec_list=[]\n","recall_list=[]\n","f1_list=[]\n","confusion_list=[]\n","M=range(0,len(fold_list),1)\n","for n,m in zip(tqdm(M),range(0,len(M))):\n","  k=fold_list[n][2]\n","  j=fold_list[n][3]\n","  dataset_t=fold_list[n][0]\n","  dataset_v=fold_list[n][1]\n","  with open(\"/content/data/results/results_{}_{}.json\".format(j,k),\"r\") as file:\n","    L=json.load(file)\n","  epoch=np.argmin(L[1])\n","  net= Classifier_PD(ngpu).to(device)\n","  net.load_state_dict(torch.load(\"/content/data/results/net_\"+str(j)+\"_\"+str(k)+\"/net_\"+str(j)+\"_\"+str(k)+\"_epoch_\"+str(epoch)+\".pth\"))\n","  dataloader_t = torch.utils.data.DataLoader(dataset_t,batch_size=batch_size,shuffle=True,num_workers=num_workers,worker_init_fn=seed_worker,generator=g, drop_last=True)\n","  dataloader_v = torch.utils.data.DataLoader(dataset_v,batch_size=batch_size,shuffle=True,num_workers=num_workers,worker_init_fn=seed_worker,generator=g, drop_last=True)\n","  trsh=0.525\n","  net.eval()\n","  confusion=np.zeros((2,2))\n","  length_dsv=0\n","  for i, datal in enumerate(dataloader_v, 0):\n","        confusiont=conf_mat(net,datal,trsh)\n","        confusion+=confusiont\n","        length_dsv+=batch_size\n","  TP=confusion[0,0]\n","  TN=confusion[1,1]\n","  FN=confusion[1,0]\n","  FP=confusion[0,1]\n","  acc=(TP+TN)/(TP+FP+FN+TN)\n","  precision=TP/(TP+FP)\n","  recall=TP/(TP+FN)\n","  F1score=(2*recall*precision)/(recall+precision)\n","  acc_list.append(acc)\n","  prec_list.append(precision)\n","  recall_list.append(recall)\n","  f1_list.append(F1score)\n","  confusion_list.append(100*confusion/length_dsv)\n","\n","confusionmean=np.round(np.mean(confusion_list,axis=0),3)\n","confusionstd=np.round(np.std(confusion_list,axis=0),3)\n","annot_confusion=np.array([str(a)+\"+/-\"+str(b) for a,b in zip(confusionmean.reshape(-1).tolist(),confusionstd.reshape(-1).tolist())]).reshape(confusionmean.shape)\n","x_axis_conf = ['stress','no stress']\n","y_axis_conf = ['stress','no stress']\n","sns.set(rc={\"figure.figsize\":(15, 5)})\n","fig, axs = plt.subplots(ncols=1,figsize=(15,9))\n","sns.heatmap(confusionmean.astype('int32'), xticklabels=x_axis_conf, yticklabels=y_axis_conf,annot=annot_confusion,ax=axs,fmt = '')\n","axs.set_xlabel('Ground Truth')\n","axs.set_ylabel('Prediction')\n","axs.title.set_text('Confusion Matrix label : Stress/No Stress (population in %)')"]},{"cell_type":"markdown","metadata":{},"source":["# Metrics"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["mes=acc_list\n","plt.hist(mes,bins=20)\n","plt.title('Accuracy of all best models')\n","plt.xlabel('Accuracy') \n","plt.ylabel('Number of models')\n","txtm=\"mean: \" +str(round(np.mean(mes),3))\n","txtstd=\"std: \" +str(round(np.std(mes),3))\n","plt.text(0.4, 20, txtm)\n","plt.text(0.4, 17, txtstd)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["mes=prec_list\n","plt.hist(mes,bins=20)\n","plt.title('Precision of all best models')\n","plt.xlabel('Precision') \n","plt.ylabel('Number of models')\n","txtm=\"mean: \" +str(round(np.mean(mes),3))\n","txtstd=\"std: \" +str(round(np.std(mes),3))\n","plt.text(0.2, 18, txtm)\n","plt.text(0.2, 16, txtstd)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["mes=recall_list\n","plt.hist(mes,bins=20)\n","plt.title('Recall of all best models')\n","plt.xlabel('Recall') \n","plt.ylabel('Number of models')\n","txtm=\"mean: \" +str(round(np.mean(mes),3))\n","txtstd=\"std: \" +str(round(np.std(mes),3))\n","plt.text(0.2, 35, txtm)\n","plt.text(0.2, 31, txtstd)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["mes=f1_list\n","plt.hist(mes,bins=20)\n","plt.title('F1 score of all best models')\n","plt.xlabel('F1 score') \n","plt.ylabel('Number of models')\n","txtm=\"mean: \" +str(round(np.mean(mes),3))\n","txtstd=\"std: \" +str(round(np.std(mes),3))\n","plt.text(0.3, 22, txtm)\n","plt.text(0.3, 16, txtstd)\n","plt.show()"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyMyvpZF99o10ouoqSQG7zUl","collapsed_sections":["USaorKo7DP9B","o4lE6XdnkRCt","VZVLgHmjVhGa"],"machine_shape":"hm","name":"Pd_classifier","provenance":[{"file_id":"1kvPKXeIcAoLtO_N66pLxRbw-k5PRgFuU","timestamp":1646091491863},{"file_id":"16DbLIaJKMKs6INHP1tJ_d7p_rPw2OXjP","timestamp":1645823537065},{"file_id":"1qTsAzF2xab3lOSky0DXfIoUUiqD1UHWF","timestamp":1645763189709}]},"kernelspec":{"display_name":"Python 3.9.7 ('venv': venv)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"vscode":{"interpreter":{"hash":"c4d2f03a047badaceda7a8cc8b96576925a546f24957406240eb0dc2593b453f"}}},"nbformat":4,"nbformat_minor":0}
